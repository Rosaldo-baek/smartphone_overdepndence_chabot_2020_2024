# =========================================================
# Streamlit ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ RAG ì±—ë´‡ v4
# - Hugging Face Hubì—ì„œ Chroma DB ë‹¤ìš´ë¡œë“œ
# - ì‚¬ìš©ì ê°€ì´ë“œ í‘œì‹œ
# =========================================================
import streamlit as st
import json
import re
import os
import pandas as pd
import shutil
from typing import Dict, Any, List, Optional, TypedDict

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# =========================================================
# í˜ì´ì§€ ì„¤ì •
# =========================================================
st.set_page_config(
    page_title="ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ì±—ë´‡",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =========================================================
# ì»¤ìŠ¤í…€ CSS
# =========================================================
st.markdown("""
<style>
    .main .block-container {
        padding-top: 1rem;
        padding-bottom: 2rem;
    }
    
    .guide-box {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.2rem;
        border-radius: 12px;
        margin-bottom: 1.5rem;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
    }
    
    .guide-title {
        font-size: 1.1rem;
        font-weight: 700;
        margin-bottom: 0.8rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
    }
    
    .guide-item {
        background: rgba(255,255,255,0.15);
        padding: 0.7rem 1rem;
        border-radius: 8px;
        margin-bottom: 0.5rem;
        font-size: 0.9rem;
        line-height: 1.5;
    }
    
    .guide-item:last-child {
        margin-bottom: 0;
    }
    
    .guide-icon {
        font-weight: bold;
        margin-right: 0.3rem;
    }
    
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffc107;
        color: #856404;
        padding: 0.8rem 1rem;
        border-radius: 8px;
        margin-bottom: 1rem;
        font-size: 0.85rem;
    }
    
    .status-box {
        background-color: #e3f2fd;
        padding: 0.8rem 1rem;
        border-radius: 8px;
        border-left: 4px solid #2196f3;
        margin: 0.5rem 0;
        font-weight: 500;
    }
    
    .user-message {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        border-left: 4px solid #1976d2;
    }
    
    .assistant-message {
        background-color: #f5f5f5;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        border-left: 4px solid #424242;
    }
    
    .dataframe {
        font-size: 14px !important;
    }
    
    h1 {
        color: #1a237e;
    }
    
    .stChatMessage {
        padding: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# =========================================================
# ìƒìˆ˜ ì„¤ì •
# =========================================================
YEAR_TO_FILENAME = {
    2020: "2020ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°_ì‚¬ë³´ê³ ì„œ.pdf",
    2021: "2021ë…„_ìŠ¤ë§ˆíŠ¸_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³´ê³ ì„œ.pdf",
    2022: "2022ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³´ê³ ì„œ.pdf",
    2023: "2023ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´ì‹¤íƒœì¡°ì‚¬_ìµœì¢…ë³´ê³ ì„œ.pdf",
    2024: "2024_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³¸_ë³´ê³ ì„œ.pdf",
}
ALLOWED_FILES = list(YEAR_TO_FILENAME.values())

BOT_IDENTITY = """2020~2024ë…„ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**ì œê³µ ê°€ëŠ¥í•œ ì •ë³´:**
- ì—°ë„ë³„ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ìœ„í—˜êµ° ë¹„ìœ¨ ë° ì¶”ì´
- ëŒ€ìƒë³„(ìœ ì•„ë™, ì²­ì†Œë…„, ì„±ì¸, 60ëŒ€) ê³¼ì˜ì¡´ í˜„í™©
- í•™ë ¹ë³„(ì´ˆ/ì¤‘/ê³ /ëŒ€í•™ìƒ) ì„¸ë¶€ ë¶„ì„
- ê³¼ì˜ì¡´ ê´€ë ¨ ìš”ì¸ ë¶„ì„ (SNS, ìˆí¼, ê²Œì„ ì´ìš© ë“±)
- ì¡°ì‚¬ ë°©ë²•ë¡  ë° í‘œë³¸ ì„¤ê³„ ì •ë³´
"""

# =========================================================
# Hugging Face ì„¤ì •
# =========================================================
HF_REPO_ID = "Rosaldowithbaek/smartphone-addiction-chroma-db"
LOCAL_DB_PATH = "./chroma_db_store"

# ê²€ìƒ‰ íŒŒë¼ë¯¸í„° (v4)
N_QUERIES = 3
K_PER_QUERY = 10
TOP_PARENTS = 15
TOP_PARENTS_PER_FILE = 5
MAX_CHUNKS_PER_PARENT = 5
MAX_CHARS_PER_DOC = 10000
SUMMARY_TYPES = ["page_summary", "table_summary"]

# í‚¤ì›Œë“œ ë¶„ë¥˜
TARGET_KEYWORDS = {
    "ëŒ€ìƒ": ["ì²­ì†Œë…„", "ìœ ì•„ë™", "ì„±ì¸", "60ëŒ€", "ì „ì²´"],
    "í•™ë ¹": ["ìœ ì¹˜ì›ìƒ", "ì´ˆë“±í•™ìƒ", "ì¤‘í•™ìƒ", "ê³ ë“±í•™ìƒ", "ëŒ€í•™ìƒ"],
    "ì„±ë³„": ["ë‚¨ì„±", "ì—¬ì„±", "ë‚¨ì", "ì—¬ì"],
    "ì§€ì—­": ["ëŒ€ë„ì‹œ", "ì¤‘ì†Œë„ì‹œ", "ìë©´ì§€ì—­", "ì/ë©´"],
    "ìœ„í—˜êµ°": ["ê³¼ì˜ì¡´ìœ„í—˜êµ°", "ì¼ë°˜ì‚¬ìš©ìêµ°", "ê³ ìœ„í—˜êµ°", "ì ì¬ì ìœ„í—˜êµ°"],
}

TOPIC_KEYWORDS = {
    "ì½˜í…ì¸ ": ["ìˆí¼", "SNS", "ê²Œì„", "ë™ì˜ìƒ", "ë©”ì‹ ì €", "ìœ íŠœë¸Œ", "í‹±í†¡", "ì¸ìŠ¤íƒ€ê·¸ë¨"],
    "ì§€í‘œ": ["ê³¼ì˜ì¡´ë¥ ", "ê³¼ì˜ì¡´", "ì´ìš©ë¥ ", "ì´ìš©ì‹œê°„", "ë¹„ìœ¨", "ì¶”ì´"],
    "ìš”ì¸": ["ê°€êµ¬ì›", "ì†Œë“", "ë§ë²Œì´", "í•œë¶€ëª¨"],
    "ì¡°ì‚¬": ["ì¡°ì‚¬ë°©ë²•", "í‘œë³¸", "ì„¤ê³„", "ì²™ë„", "í‘œë³¸ì„¤ê³„"],
}

# =========================================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =========================================================
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# =========================================================
# LangGraph State ì •ì˜
# =========================================================
class GraphState(TypedDict):
    input: str
    chat_history: List[BaseMessage]
    session_id: str
    intent_raw: Optional[str]
    intent: Optional[str]
    is_chat_reference: Optional[bool]
    followup_type: Optional[str]
    plan: Optional[Dict[str, Any]]
    resolved_question: Optional[str]
    previous_context: Optional[str]
    retrieval: Optional[Dict[str, Any]]
    context: Optional[str]
    draft_answer: Optional[str]
    validator_result: Optional[Dict[str, Any]]
    final_answer: Optional[str]
    debug_info: Optional[Dict[str, Any]]

# =========================================================
# Hugging Faceì—ì„œ DB ë‹¤ìš´ë¡œë“œ
# =========================================================
@st.cache_resource
def download_chroma_db():
    """Hugging Face Hubì—ì„œ Chroma DB ë‹¤ìš´ë¡œë“œ"""
    if os.path.exists(LOCAL_DB_PATH) and os.listdir(LOCAL_DB_PATH):
        return LOCAL_DB_PATH, None
    
    try:
        from huggingface_hub import snapshot_download
        
        downloaded_path = snapshot_download(
            repo_id=HF_REPO_ID,
            repo_type="dataset",
            local_dir=LOCAL_DB_PATH,
            local_dir_use_symlinks=False
        )
        return downloaded_path, None
    except Exception as e:
        return None, str(e)

# =========================================================
# ì´ˆê¸°í™” í•¨ìˆ˜
# =========================================================
@st.cache_resource
def init_resources():
    """ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”"""
    api_key = None
    
    try:
        api_key = st.secrets.get("OPENAI_API_KEY")
    except:
        pass
    
    if not api_key:
        api_key = os.environ.get("OPENAI_API_KEY")
    
    if not api_key:
        return None, None, "API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    os.environ['OPENAI_API_KEY'] = api_key
    
    if not os.path.exists(LOCAL_DB_PATH):
        return None, None, f"Chroma DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LOCAL_DB_PATH}"
    
    try:
        embedding = OpenAIEmbeddings(model='text-embedding-3-large')
        vectorstore = Chroma(
            persist_directory=LOCAL_DB_PATH,
            embedding_function=embedding,
            collection_name="pdf_pages_with_summary_v2"
        )
        
        llms = {
            "router": ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=50),
            "casual": ChatOpenAI(model="gpt-4o-mini", temperature=0.5, max_tokens=500),
            "main": ChatOpenAI(model="gpt-4o", temperature=0.2, max_tokens=4000),
            "planner": ChatOpenAI(model="gpt-4o", temperature=0, max_tokens=1000),
        }
        
        return vectorstore, llms, None
    except Exception as e:
        return None, None, str(e)

# =========================================================
# í—¬í¼ í•¨ìˆ˜ë“¤
# =========================================================
def is_chat_reference_question(user_input: str) -> bool:
    name_intro_patterns = [
        r"(ë‚´|ì œ)\s*ì´ë¦„ì€?\s*[ê°€-í£a-zA-Z]+",
        r"(ì €ëŠ”|ë‚˜ëŠ”)\s*[ê°€-í£a-zA-Z]+",
    ]
    for p in name_intro_patterns:
        if re.search(p, user_input):
            return False
    
    patterns = [
        r"(ë‚´|ì œ)\s*ì´ë¦„\s*(ë­|ë­”|ì•Œ|ê¸°ì–µ)",
        r"(ë‚´|ì œ)\s*ì´ë¦„\s*[?]",
        r"ë­ë¼ê³ \s*(í–ˆ|ë¬¼ì–´|ë§)",
        r"ì•„ê¹Œ", r"ë°©ê¸ˆ", r"ì´ì „ì—",
    ]
    for p in patterns:
        if re.search(p, user_input):
            return True
    return False

def parse_year_range(text: str) -> List[int]:
    years = set()
    range_patterns = [
        r"(20[2][0-4])\s*ë…„?\s*(?:ì—ì„œ|ë¶€í„°|~|-|â€“)\s*(20[2][0-4])\s*ë…„?\s*(?:ê¹Œì§€)?",
        r"(20[2][0-4])\s*(?:~|-|â€“)\s*(20[2][0-4])",
    ]
    for pattern in range_patterns:
        matches = re.findall(pattern, text)
        for m in matches:
            start, end = int(m[0]), int(m[1])
            for y in range(start, end + 1):
                if y in YEAR_TO_FILENAME:
                    years.add(y)
    
    single_years = re.findall(r"\b(20[2][0-4])\s*ë…„?\b", text)
    for y in single_years:
        yi = int(y)
        if yi in YEAR_TO_FILENAME:
            years.add(yi)
    
    return sorted(list(years))

def classify_followup_type(user_input: str, prev_context: Dict[str, Any]) -> str:
    user_input_clean = user_input.strip()
    
    if not prev_context.get("last_topic"):
        return "none"
    
    has_new_topic_keyword = False
    for category, keywords in TOPIC_KEYWORDS.items():
        for kw in keywords:
            if kw in user_input and kw not in str(prev_context.get("last_topic_core", "")):
                has_new_topic_keyword = True
                break
    
    if len(user_input) >= 30 and has_new_topic_keyword:
        return "none"
    
    target_patterns = [
        r"^(ì²­ì†Œë…„|ìœ ì•„ë™|ì„±ì¸|60ëŒ€|ëŒ€í•™ìƒ|ì¤‘í•™ìƒ|ê³ ë“±í•™ìƒ|ì´ˆë“±í•™ìƒ|ë‚¨ì„±|ì—¬ì„±)[ì€ì˜]?\s*[?]?$",
        r"^(ì²­ì†Œë…„|ìœ ì•„ë™|ì„±ì¸|60ëŒ€)[ì€ì˜]?\s*(ì–´ë•Œ|ì–´ë–»ê²Œ|ì–´ë–¤ê°€|ê²°ê³¼|ê¸°ì¤€|ê²½ìš°)",
        r"(ì²­ì†Œë…„|ìœ ì•„ë™|ì„±ì¸|60ëŒ€)[ì€ì˜]?\s*(ì–´ë•Œ|ì–´ë–»ê²Œ|ì–´ë–¤ê°€)\s*[?]?$",
    ]
    for p in target_patterns:
        if re.search(p, user_input):
            return "target_change"
    
    if len(user_input) <= 20:
        for keywords in TARGET_KEYWORDS.values():
            for kw in keywords:
                if kw in user_input:
                    return "target_change"
    
    year_patterns = [
        r"^(20[2][0-4])ë…„?\s*[ì€ì˜]?\s*[?]?$",
        r"^(20[2][0-4])ë…„?\s*(ì–´ë•Œ|ì–´ë–»ê²Œ|ê²°ê³¼|ê¸°ì¤€)",
        r"ê·¸\s*(ì—°ë„|í•´|ë…„ë„)[ëŠ”ì€]?",
    ]
    for p in year_patterns:
        if re.search(p, user_input):
            return "year_change"
    
    if len(user_input) <= 15:
        years = parse_year_range(user_input)
        if years:
            return "year_change"
    
    detail_patterns = [
        r"(ë”|ì¢€)\s*(ìì„¸íˆ|êµ¬ì²´ì |ìƒì„¸)",
        r"(ì™œ|ì›ì¸|ì´ìœ ).*[?]",
        r"(ì–´ë–¤|ë¬´ìŠ¨)\s*(ìš”ì¸|ì´ìœ |ì›ì¸)",
        r"^(ê·¸ë˜ì„œ|ê·¸ëŸ¬ë©´|ê·¸ëŸ¼)\s*[?]?$",
    ]
    for p in detail_patterns:
        if re.search(p, user_input):
            return "detail_request"
    
    if len(user_input) <= 15 and re.search(r"[?]$", user_input):
        return "detail_request"
    
    return "none"

def extract_previous_context(chat_history: List[BaseMessage]) -> Dict[str, Any]:
    context = {
        "user_name": None,
        "last_topic": None,
        "last_topic_core": None,
        "last_target": None,
        "last_years": [],
    }
    
    if not chat_history:
        return context
    
    for msg in chat_history:
        if isinstance(msg, HumanMessage):
            name_match = re.search(r"(?:ë‚´\s*ì´ë¦„ì€?|ì €ëŠ”?|ë‚˜ëŠ”?)\s*([ê°€-í£a-zA-Z]+)", msg.content)
            if name_match:
                context["user_name"] = name_match.group(1)
    
    human_msgs = [m for m in chat_history if isinstance(m, HumanMessage)][-2:]
    
    for msg in reversed(human_msgs):
        content = msg.content
        
        if not context["last_topic"]:
            context["last_topic"] = content[:300]
        
        years = parse_year_range(content)
        if years and not context["last_years"]:
            context["last_years"] = years
        
        if not context["last_target"]:
            for keywords in TARGET_KEYWORDS.values():
                for kw in keywords:
                    if kw in content:
                        context["last_target"] = kw
                        break
                if context["last_target"]:
                    break
        
        if not context["last_topic_core"]:
            topic_parts = []
            for keywords in TOPIC_KEYWORDS.values():
                for kw in keywords:
                    if kw in content:
                        topic_parts.append(kw)
            if topic_parts:
                context["last_topic_core"] = " ".join(topic_parts[:3])
    
    return context

def _keyword_boost_score(doc: Document, query: str) -> float:
    text = (doc.page_content or "").lower()
    query_terms = re.findall(r'[ê°€-í£a-zA-Z0-9]+', query.lower())
    boost = 0.0
    for term in query_terms:
        if len(term) >= 2 and term in text:
            boost += 0.02
    return min(boost, 0.15)

# =========================================================
# í…Œì´ë¸” íŒŒì‹± ë° ë Œë”ë§
# =========================================================
def parse_markdown_table(text: str) -> List[Dict[str, Any]]:
    tables = []
    lines = text.split('\n')
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if line.startswith('|') and line.endswith('|'):
            table_lines = []
            start_idx = i
            while i < len(lines):
                line = lines[i].strip()
                if line.startswith('|') and line.endswith('|'):
                    table_lines.append(line)
                    i += 1
                elif line.startswith('|---') or line.startswith('| ---'):
                    i += 1
                    continue
                else:
                    break
            
            if len(table_lines) >= 2:
                header_line = table_lines[0]
                headers = [h.strip() for h in header_line.split('|')[1:-1]]
                data_rows = []
                for row_line in table_lines[1:]:
                    if '---' in row_line:
                        continue
                    cells = [c.strip() for c in row_line.split('|')[1:-1]]
                    if len(cells) == len(headers):
                        data_rows.append(cells)
                
                if headers and data_rows:
                    tables.append({
                        'headers': headers,
                        'rows': data_rows,
                        'start_idx': start_idx,
                        'end_idx': i
                    })
        else:
            i += 1
    return tables

def render_answer_with_tables(answer: str) -> None:
    tables = parse_markdown_table(answer)
    if not tables:
        st.markdown(answer)
        return
    
    lines = answer.split('\n')
    current_pos = 0
    
    for table in tables:
        before_text = '\n'.join(lines[current_pos:table['start_idx']])
        if before_text.strip():
            st.markdown(before_text)
        
        try:
            df = pd.DataFrame(table['rows'], columns=table['headers'])
            st.dataframe(df, use_container_width=True, hide_index=True)
        except:
            st.markdown("| " + " | ".join(table['headers']) + " |")
            for row in table['rows']:
                st.markdown("| " + " | ".join(row) + " |")
        
        current_pos = table['end_idx']
    
    after_text = '\n'.join(lines[current_pos:])
    if after_text.strip():
        st.markdown(after_text)

# =========================================================
# í”„ë¡¬í”„íŠ¸ ì •ì˜
# =========================================================
def get_router_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.\n"
         "ì´ ì‹œìŠ¤í…œì€ 'ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024)' ì „ë¬¸ RAGì…ë‹ˆë‹¤.\n\n"
         "ë¶„ë¥˜ ê¸°ì¤€ (í•˜ë‚˜ë§Œ ì„ íƒ):\n"
         "SMALLTALK: ì¸ì‚¬, ì‹œìŠ¤í…œ ì§ˆë¬¸\n"
         "RAG: ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ê´€ë ¨ ì§ˆë¬¸\n"
         "CHAT_REF: ì´ì „ ëŒ€í™” ì°¸ì¡°\n"
         "OFFTOPIC: ì™„ì „íˆ ê´€ë ¨ ì—†ëŠ” ì£¼ì œ\n\n"
         "ì¶œë ¥: ë¶„ë¥˜ëª…ë§Œ"
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

def get_smalltalk_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         f"ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024ë…„) ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n\n"
         f"ì‹œìŠ¤í…œ ì—­í• :\n{BOT_IDENTITY}\n\n"
         "ì‘ë‹µ ì§€ì¹¨:\n"
         "- ì¸ì‚¬ì—ëŠ” ê°„ê²°í•˜ê²Œ ì‘ëŒ€\n"
         "- ì‚¬ìš©ìê°€ ì´ë¦„ì„ ì†Œê°œí•˜ë©´ '{{ì´ë¦„}}ë‹˜, ë°˜ê°‘ìŠµë‹ˆë‹¤'ë¡œ ì‘ëŒ€\n"
         "- ì—­í•  ì†Œê°œ ì‹œ ì˜ˆì‹œ ì§ˆë¬¸ ì œì•ˆ\n"
         "- ì´ëª¨í‹°ì½˜ ê¸ˆì§€, ê²©ì‹ì²´ ì‚¬ìš©"
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

def get_offtopic_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n"
         "í•´ë‹¹ ì§ˆë¬¸ì€ ì „ë¬¸ ë¶„ì•¼ê°€ ì•„ë‹™ë‹ˆë‹¤.\n"
         "ì •ì¤‘í•˜ê²Œ ì•ˆë‚´í•˜ê³ , ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ê´€ë ¨ ì§ˆë¬¸ì€ ë„ì›€ ê°€ëŠ¥í•˜ë‹¤ê³  ì•Œë ¤ì£¼ì„¸ìš”.\n"
         "ì´ëª¨í‹°ì½˜ ê¸ˆì§€, ê°„ê²°í•˜ê²Œ."
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

def get_planner_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024ë…„) ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ê¸°ì…ë‹ˆë‹¤.\n"
         "ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
         "í›„ì†ì§ˆë¬¸ ìœ í˜•ë³„ ì²˜ë¦¬:\n"
         "- followup_type='none': ì´ì „ ë§¥ë½ ë¬´ì‹œ\n"
         "- followup_type='target_change': ì´ì „ ì£¼ì œ ìœ ì§€ + ìƒˆ ëŒ€ìƒ\n"
         "- followup_type='year_change': ì´ì „ ì£¼ì œ ìœ ì§€ + ìƒˆ ì—°ë„\n"
         "- followup_type='detail_request': ì´ì „ ë§¥ë½ ì „ì²´ ìœ ì§€\n\n"
         "ë©€í‹°ì—°ë„ ì¿¼ë¦¬ ìƒì„±: ê° ì—°ë„ë³„ë¡œ êµ¬ì²´ì ì¸ ì¿¼ë¦¬ í¬í•¨\n\n"
         "í—ˆìš© íŒŒì¼ëª…:\n" +
         "\n".join([f"- {y}ë…„: {fn}" for y, fn in YEAR_TO_FILENAME.items()]) +
         "\n\nJSON ìŠ¤í‚¤ë§ˆ:\n"
         "{{\n"
         '  "resolved_question": "ì™„ì „í•œ ì§ˆë¬¸",\n'
         '  "years": [2020, ...],\n'
         '  "file_name_filters": ["íŒŒì¼ëª…"],\n'
         '  "queries": ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", "ì¿¼ë¦¬3"]\n'
         "}}"
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", 
         "í˜„ì¬ ì§ˆë¬¸: {input}\n"
         "í›„ì†ì§ˆë¬¸ ìœ í˜•: {followup_type}\n"
         "ì´ì „ í•µì‹¬ ì£¼ì œ: {topic_core}\n"
         "ì´ì „ ëŒ€ìƒ: {last_target}\n"
         "ì´ì „ ì—°ë„: {last_years}\n\nJSON:")
    ])

def get_answer_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024ë…„) ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n\n"
         "í•µì‹¬ ì›ì¹™:\n"
         "1. CONTEXTì— ìˆëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜/ë¹„ìœ¨ì„ ë°˜ë“œì‹œ ì¸ìš©\n"
         "2. ëª¨ë“  ìˆ˜ì¹˜ì—ëŠ” ì¶œì²˜(íŒŒì¼ëª… p.í˜ì´ì§€) í•„ìˆ˜\n"
         "3. ì—°ë„ë³„ ë¹„êµ ì‹œ ë³€í™”ëŸ‰(%p) ëª…ì‹œ\n"
         "4. ê°ê´€ì ì´ê³  ë‹´ë°±í•œ í†¤\n\n"
         "í˜•ì‹:\n"
         "- í•µì‹¬ ìˆ˜ì¹˜ë¥¼ ë¨¼ì € ì œì‹œ\n"
         "- ì—°ë„ë³„/ëŒ€ìƒë³„ ë°ì´í„°ëŠ” í‘œ í˜•ì‹ ê¶Œì¥\n"
         "- ì´ëª¨í‹°ì½˜ ê¸ˆì§€, ê²©ì‹ì²´ ì‚¬ìš©\n\n"
         "ì¤‘ìš”:\n"
         "- CONTEXTì— ì—†ëŠ” ì—°ë„/í•­ëª©ì€ 'í•´ë‹¹ ë°ì´í„°ëŠ” ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'ë¡œ ëª…ì‹œ\n"
         "- ì¶”ì¸¡ ê¸ˆì§€, ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€"
        ),
        ("human",
         "[ì§ˆë¬¸]\n{input}\n\n"
         "[ê²€ìƒ‰ ê²°ê³¼ (CONTEXT)]\n{context}\n\n"
         "ìœ„ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.")
    ])

def get_validator_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "í†µê³„ ë³´ê³ ì„œ ë‹µë³€ í’ˆì§ˆ ê²€ìˆ˜ê¸°ì…ë‹ˆë‹¤.\n\n"
         "ê²€ìˆ˜ í•­ëª©:\n"
         "1. ìˆ˜ì¹˜ì— ì¶œì²˜ ìˆëŠ”ì§€\n"
         "2. CONTEXTì— ì—†ëŠ” ìˆ˜ì¹˜ ìƒì„±í–ˆëŠ”ì§€\n"
         "3. ìš”ì²­í•œ ì—°ë„/ëŒ€ìƒ ëª¨ë‘ ë‹¤ë¤˜ëŠ”ì§€\n\n"
         "JSONë§Œ ì¶œë ¥:\n"
         "{{\n"
         '  "needs_fix": true|false,\n'
         '  "issues": ["ë¬¸ì œì "],\n'
         '  "corrected_answer": "ìˆ˜ì •ëœ ë‹µë³€"\n'
         "}}"
        ),
        ("human",
         "[ì§ˆë¬¸]\n{input}\n\n"
         "[ê²€ìƒ‰ ê²°ê³¼]\n{context}\n\n"
         "[ë‹µë³€]\n{answer}\n\nJSON:")
    ])

# =========================================================
# ë…¸ë“œ í•¨ìˆ˜ë“¤
# =========================================================
def create_node_functions(vectorstore, llms, status_placeholder):
    
    def update_status(message: str):
        status_placeholder.markdown(f"""
        <div class="status-box">ğŸ”„ {message}</div>
        """, unsafe_allow_html=True)
    
    def route_intent(state: GraphState) -> GraphState:
        update_status("ì§ˆë¬¸ ë¶„ì„ ì¤‘...")
        try:
            user_input = state["input"]
            chat_history = state.get("chat_history", [])
            
            if is_chat_reference_question(user_input):
                state["intent"] = "CHAT_REF"
                state["is_chat_reference"] = True
                state["followup_type"] = "none"
                return state
            
            prev_ctx = extract_previous_context(chat_history)
            followup_type = classify_followup_type(user_input, prev_ctx)
            state["followup_type"] = followup_type
            
            rag_keywords = [
                "ê³¼ì˜ì¡´", "ìŠ¤ë§ˆíŠ¸í°", "ì¡°ì‚¬", "ì‹¤íƒœ", "ë¹„ìœ¨", "ë¥ ", "%",
                "í†µê³„", "ìˆ˜ì¹˜", "ê²°ê³¼", "ì²­ì†Œë…„", "ëŒ€í•™ìƒ", "ì„±ì¸", "ìœ ì•„ë™",
                "ìˆí¼", "SNS", "ê²Œì„", "ì´ìš©ë¥ ", "ìœ„í—˜êµ°", "60ëŒ€",
                "ì´ˆë“±í•™ìƒ", "ì¤‘í•™ìƒ", "ê³ ë“±í•™ìƒ"
            ]
            
            if re.search(r"\b(20[2][0-4])\s*ë…„?\b", user_input):
                state["intent"] = "RAG"
                return state
            
            if any(kw in user_input for kw in rag_keywords):
                state["intent"] = "RAG"
                return state
            
            if followup_type != "none":
                state["intent"] = "RAG"
                return state
            
            result = (get_router_prompt() | llms["router"] | StrOutputParser()).invoke({
                "input": user_input,
                "chat_history": chat_history
            })
            state["intent_raw"] = result.strip().upper()
            
            if state["intent_raw"] in ("SMALLTALK", "RAG", "OFFTOPIC", "CHAT_REF"):
                state["intent"] = state["intent_raw"]
            else:
                state["intent"] = "RAG"
            
            return state
        except Exception as e:
            state["intent"] = "RAG"
            state["followup_type"] = "none"
            return state
    
    def handle_smalltalk(state: GraphState) -> GraphState:
        update_status("ì‘ë‹µ ìƒì„± ì¤‘...")
        try:
            answer = (get_smalltalk_prompt() | llms["casual"] | StrOutputParser()).invoke({
                "input": state["input"],
                "chat_history": state.get("chat_history", [])
            })
            state["final_answer"] = answer
            return state
        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state
    
    def handle_offtopic(state: GraphState) -> GraphState:
        update_status("ì‘ë‹µ ìƒì„± ì¤‘...")
        try:
            answer = (get_offtopic_prompt() | llms["casual"] | StrOutputParser()).invoke({
                "input": state["input"],
                "chat_history": state.get("chat_history", [])
            })
            state["final_answer"] = answer
            return state
        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state
    
    def handle_chat_reference(state: GraphState) -> GraphState:
        update_status("ëŒ€í™” ê¸°ë¡ í™•ì¸ ì¤‘...")
        try:
            chat_history = state.get("chat_history", [])
            user_input = state["input"]
            prev_ctx = extract_previous_context(chat_history)
            
            if re.search(r"(ë‚´|ì œ)\s*ì´ë¦„", user_input):
                if prev_ctx["user_name"]:
                    state["final_answer"] = f"{prev_ctx['user_name']}ë‹˜ìœ¼ë¡œ ë§ì”€í•˜ì…¨ìŠµë‹ˆë‹¤."
                else:
                    state["final_answer"] = "ì•„ì§ ì´ë¦„ì„ ë§ì”€í•´ì£¼ì‹œì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                return state
            
            if re.search(r"(ë­ë¼ê³ |ë¬´ìŠ¨\s*ë§)", user_input):
                if prev_ctx["last_topic"]:
                    state["final_answer"] = f"ì´ì „ì— '{prev_ctx['last_topic'][:80]}...'ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì…¨ìŠµë‹ˆë‹¤."
                else:
                    state["final_answer"] = "ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                return state
            
            state["final_answer"] = "ì´ì „ ëŒ€í™” ì°¸ì¡°ê°€ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            return state
        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state
    
    def plan_search(state: GraphState) -> GraphState:
        update_status("ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ ì¤‘...")
        try:
            user_input = state["input"]
            chat_history = state.get("chat_history", [])
            followup_type = state.get("followup_type", "none")
            
            prev_ctx = extract_previous_context(chat_history)
            
            if followup_type == "none":
                topic_core = ""
                last_target = ""
                last_years = []
            else:
                topic_core = prev_ctx.get("last_topic_core", "") or ""
                last_target = prev_ctx.get("last_target", "") or ""
                last_years = prev_ctx.get("last_years", [])
            
            state["previous_context"] = f"type={followup_type}, topic={topic_core}"
            
            result = (get_planner_prompt() | llms["planner"] | StrOutputParser()).invoke({
                "input": user_input,
                "chat_history": chat_history[-4:] if len(chat_history) > 4 else chat_history,
                "followup_type": followup_type,
                "topic_core": topic_core,
                "last_target": last_target,
                "last_years": str(last_years),
            })
            
            json_match = re.search(r'\{[\s\S]*\}', result)
            if json_match:
                result = json_match.group()
            
            plan = json.loads(result)
            
            years = plan.get('years', [])
            if not isinstance(years, list):
                years = []
            
            input_years = parse_year_range(user_input)
            years = list(set(years + input_years))
            years = [y for y in years if isinstance(y, int) and y in YEAR_TO_FILENAME]
            
            if followup_type == "year_change" and not years and last_years:
                years = last_years
            
            years = sorted(years)
            
            fns = plan.get("file_name_filters", [])
            if not isinstance(fns, list):
                fns = []
            fns = [fn for fn in fns if isinstance(fn, str) and fn in ALLOWED_FILES]
            
            if years and not fns:
                fns = [YEAR_TO_FILENAME[y] for y in years if y in YEAR_TO_FILENAME]
            
            queries = plan.get('queries', [])
            if not isinstance(queries, list):
                queries = []
            queries = [str(q).strip() for q in queries if str(q).strip()]
            
            resolved_q = plan.get("resolved_question", user_input)
            if not isinstance(resolved_q, str) or not resolved_q.strip():
                resolved_q = user_input
            
            while len(queries) < N_QUERIES:
                queries.append(resolved_q)
            queries = queries[:N_QUERIES]
            
            state["plan"] = {
                "years": years,
                "file_name_filters": fns,
                "queries": queries,
                "resolved_question": resolved_q,
                "followup_type": followup_type,
            }
            state["resolved_question"] = resolved_q
            
            return state
            
        except Exception as e:
            years = parse_year_range(state["input"])
            fns = [YEAR_TO_FILENAME[y] for y in years if y in YEAR_TO_FILENAME]
            
            state["plan"] = {
                "years": years,
                "file_name_filters": fns,
                "queries": [state["input"]] * N_QUERIES,
                "resolved_question": state["input"],
                "followup_type": "none",
            }
            state["resolved_question"] = state["input"]
            return state
    
    def retrieve_documents(state: GraphState) -> GraphState:
        update_status("ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘...")
        try:
            plan = state["plan"]
            target_files = plan.get("file_name_filters", [])
            queries = plan.get("queries", [])
            resolved_q = plan.get("resolved_question", "")
            years = plan.get("years", [])
            
            # ë©€í‹°ì—°ë„ ì¿¼ë¦¬ ìë™ ì¶”ê°€
            if len(years) > 1:
                base_query_clean = re.sub(r'20[2][0-4]ë…„?', '', resolved_q).strip()
                for y in years:
                    year_query = f"{y}ë…„ {base_query_clean}"
                    if year_query not in queries:
                        queries.append(year_query)
            
            all_docs = []
            files_searched = []
            
            if target_files:
                for fn in target_files:
                    file_filter = {'$and': [
                        {'doc_type': {"$in": SUMMARY_TYPES}},
                        {'file_name': fn}
                    ]}
                    
                    file_docs = []
                    seen_keys = set()
                    
                    for q in queries:
                        if not q:
                            continue
                        try:
                            hits = vectorstore.similarity_search_with_relevance_scores(
                                q, k=K_PER_QUERY, filter=file_filter
                            )
                            for doc, score in hits:
                                key = f"{doc.metadata.get('parent_id')}|{doc.metadata.get('page')}"
                                if key in seen_keys:
                                    continue
                                doc.metadata["_score"] = float(score)
                                doc.metadata["_source_file"] = fn
                                file_docs.append(doc)
                                seen_keys.add(key)
                        except Exception as e:
                            pass
                    
                    for doc in file_docs:
                        base_score = doc.metadata.get("_score", 0.0)
                        boost = _keyword_boost_score(doc, resolved_q)
                        doc.metadata["_final_score"] = base_score + boost
                    
                    file_docs.sort(key=lambda d: d.metadata.get("_final_score", 0.0), reverse=True)
                    all_docs.extend(file_docs[:TOP_PARENTS_PER_FILE * 2])
                    
                    if file_docs:
                        files_searched.append(fn)
            else:
                base_filter = {'doc_type': {"$in": SUMMARY_TYPES}}
                seen_keys = set()
                
                for q in queries:
                    if not q:
                        continue
                    hits = vectorstore.similarity_search_with_relevance_scores(
                        q, k=K_PER_QUERY, filter=base_filter
                    )
                    for doc, score in hits:
                        key = f"{doc.metadata.get('parent_id')}|{doc.metadata.get('page')}"
                        if key in seen_keys:
                            continue
                        doc.metadata["_score"] = float(score)
                        all_docs.append(doc)
                        seen_keys.add(key)
                
                for doc in all_docs:
                    base_score = doc.metadata.get("_score", 0.0)
                    boost = _keyword_boost_score(doc, resolved_q)
                    doc.metadata["_final_score"] = base_score + boost
                
                files_searched = ["ì „ì²´"]
            
            all_docs.sort(key=lambda d: d.metadata.get("_final_score", 0.0), reverse=True)
            
            parent_ids = []
            seen_pid = set()
            
            if target_files:
                for fn in target_files:
                    for doc in all_docs:
                        if doc.metadata.get("file_name") != fn and doc.metadata.get("_source_file") != fn:
                            continue
                        pid = doc.metadata.get("parent_id")
                        if pid and pid not in seen_pid:
                            parent_ids.append(pid)
                            seen_pid.add(pid)
                            break
                
                for doc in all_docs:
                    if len(parent_ids) >= TOP_PARENTS:
                        break
                    pid = doc.metadata.get("parent_id")
                    if pid and pid not in seen_pid:
                        parent_ids.append(pid)
                        seen_pid.add(pid)
            else:
                for doc in all_docs:
                    pid = doc.metadata.get("parent_id")
                    if not pid or pid in seen_pid:
                        continue
                    parent_ids.append(pid)
                    seen_pid.add(pid)
                    if len(parent_ids) >= TOP_PARENTS:
                        break
            
            expanded_chunks = []
            for pid in parent_ids:
                try:
                    got = vectorstore._collection.get(
                        where={'parent_id': pid},
                        include=['documents', 'metadatas']
                    )
                    docs = got.get("documents", []) or []
                    metas = got.get("metadatas", []) or []
                    
                    chunks = []
                    for txt, meta in zip(docs, metas):
                        if not isinstance(meta, dict):
                            continue
                        if meta.get("doc_type") != "text_chunk":
                            continue
                        idx = int(meta.get("chunk_index", 0))
                        chunks.append((idx, txt or "", meta))
                    
                    chunks.sort(key=lambda x: x[0])
                    for idx, txt, meta in chunks[:MAX_CHUNKS_PER_PARENT]:
                        expanded_chunks.append(Document(page_content=txt, metadata=meta))
                except:
                    pass
            
            pid_set = set(parent_ids)
            kept_summaries = [d for d in all_docs if d.metadata.get("parent_id") in pid_set]
            final_docs = kept_summaries + expanded_chunks
            
            blocks = []
            for i, d in enumerate(final_docs, start=1):
                m = d.metadata
                text = d.page_content[:MAX_CHARS_PER_DOC]
                blocks.append(
                    f"[{i}] {m.get('file_name', 'unknown')} (p.{m.get('page', '?')})\n{text}"
                )
            context = "\n\n---\n\n".join(blocks)
            
            state["retrieval"] = {
                "docs": final_docs,
                "parent_ids": parent_ids,
                "files_searched": files_searched,
                "doc_count": len(final_docs),
            }
            state["context"] = context
            
            return state
            
        except Exception as e:
            state["context"] = ""
            state["retrieval"] = {"docs": [], "parent_ids": [], "files_searched": [], "doc_count": 0}
            return state
    
    def generate_answer(state: GraphState) -> GraphState:
        update_status("ë‹µë³€ ìƒì„± ì¤‘...")
        try:
            context = state.get("context", "")
            
            if not context.strip():
                state["draft_answer"] = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"
                return state
            
            answer = (get_answer_prompt() | llms["main"] | StrOutputParser()).invoke({
                "input": state["resolved_question"] or state["input"],
                "context": context
            })
            state["draft_answer"] = answer
            return state
        except Exception as e:
            state["draft_answer"] = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state
    
    def validate_answer(state: GraphState) -> GraphState:
        update_status("ë‹µë³€ ê²€ì¦ ì¤‘...")
        try:
            result = (get_validator_prompt() | llms["main"] | StrOutputParser()).invoke({
                "input": state["resolved_question"] or state["input"],
                "context": state.get("context", ""),
                "answer": state["draft_answer"]
            })
            
            json_match = re.search(r'\{[\s\S]*\}', result)
            if json_match:
                result = json_match.group()
            
            validator_out = json.loads(result)
            state["validator_result"] = validator_out
            
            if validator_out.get("needs_fix") and validator_out.get("corrected_answer"):
                state["final_answer"] = validator_out["corrected_answer"]
            else:
                state["final_answer"] = state["draft_answer"]
            
            return state
        except:
            state["final_answer"] = state["draft_answer"]
            return state
    
    def handle_clarify(state: GraphState) -> GraphState:
        clarify_msg = state["resolved_question"].replace("CLARIFY:", "", 1).strip()
        state["final_answer"] = clarify_msg
        return state
    
    return {
        "route_intent": route_intent,
        "smalltalk": handle_smalltalk,
        "offtopic": handle_offtopic,
        "chat_ref": handle_chat_reference,
        "plan_search": plan_search,
        "retrieve": retrieve_documents,
        "generate": generate_answer,
        "validate": validate_answer,
        "clarify": handle_clarify,
    }

# =========================================================
# ê·¸ë˜í”„ ë¹Œë”
# =========================================================
def build_graph(node_functions):
    workflow = StateGraph(GraphState)
    
    for name, func in node_functions.items():
        workflow.add_node(name, func)
    
    def route_by_intent(state: GraphState) -> str:
        intent = state.get("intent", "RAG")
        if intent == "SMALLTALK":
            return "smalltalk"
        elif intent == "OFFTOPIC":
            return "offtopic"
        elif intent == "CHAT_REF":
            return "chat_ref"
        else:
            return "rag_pipeline"
    
    def check_clarify(state: GraphState) -> str:
        resolved = state.get("resolved_question", "")
        if resolved.startswith("CLARIFY:"):
            return "clarify"
        return "retrieve"
    
    workflow.set_entry_point("route_intent")
    
    workflow.add_conditional_edges(
        "route_intent",
        route_by_intent,
        {
            "smalltalk": "smalltalk",
            "offtopic": "offtopic",
            "chat_ref": "chat_ref",
            "rag_pipeline": "plan_search"
        }
    )
    
    workflow.add_edge("smalltalk", END)
    workflow.add_edge("offtopic", END)
    workflow.add_edge("chat_ref", END)
    
    workflow.add_conditional_edges(
        "plan_search",
        check_clarify,
        {
            "clarify": "clarify",
            "retrieve": "retrieve"
        }
    )
    
    workflow.add_edge("clarify", END)
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", "validate")
    workflow.add_edge("validate", END)
    
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)

# =========================================================
# ë©”ì¸ UI
# =========================================================
def main():
    st.title("ğŸ“Š ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë¶„ì„ ì‹œìŠ¤í…œ")
    
    # =========================================================
    # ì‚¬ì´ë“œë°”
    # =========================================================
    with st.sidebar:
        st.header("ğŸ“‹ ì‹œìŠ¤í…œ ì •ë³´")
        st.markdown(BOT_IDENTITY)
        
        st.divider()
        
        st.subheader("ğŸ“… ë°ì´í„° ë²”ìœ„")
        for year in YEAR_TO_FILENAME.keys():
            st.caption(f"â€¢ {year}ë…„ ë³´ê³ ì„œ")
        
        st.divider()
        
        if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.messages = []
            st.session_state.chat_history = []
            st.rerun()
        
        st.divider()
        
        debug_mode = st.checkbox("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ", value=False)
        
        st.divider()
        st.caption(f"HF Repo: {HF_REPO_ID}")
    
    # =========================================================
    # âœ… ì‚¬ìš©ì ê°€ì´ë“œ ë°•ìŠ¤
    # =========================================================
    st.markdown("""
    <div class="guide-box">
        <div class="guide-title">ğŸ“Œ ì‚¬ìš© ì•ˆë‚´</div>
        <div class="guide-item">
            <strong>â„¹ï¸ ìš©ë„:</strong> ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024) <strong>ë‹¨ìˆœ ì •ë³´ ê²€ìƒ‰ìš©</strong>ì…ë‹ˆë‹¤. <br>
            ì¸ì‚¬ì´íŠ¸ ì œê³µ, ì¼ë°˜ ëŒ€í™”, ë³´ê³ ì„œ ì™¸ ì •ë³´ ê²€ìƒ‰ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        </div>
        <div class="guide-item">
            <strong>ğŸ’¡ ê²€ìƒ‰ íŒ:</strong> ì§ˆë¬¸ì€ <strong>ìµœëŒ€í•œ êµ¬ì²´ì ìœ¼ë¡œ</strong> ì‘ì„±í•´ ì£¼ì„¸ìš”.<br>
            ê³¼ë„í•œ ê²€ìƒ‰ê²°ê³¼ ë°©ì§€ë¥¼ ìœ„í•œ ì„¤ì •ìœ¼ë¡œ ì¸í•´ ì¼ë¶€ ì—°ë„ê°€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ëˆ„ë½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ´ ë•ŒëŠ” í•´ë‹¹ ì—°ë„ë¥¼ ì§€ì •í•´ì„œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.<br>
            ë³´ê³ ì„œ ë‚´ ìœ ì‚¬í•œ ë‚´ìš©ì´ ë‹¤ìˆ˜ ìˆì–´, ê²€ìƒ‰ ì„±ëŠ¥ì´ ì•ˆë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš”êµ¬í•˜ê³ ìí•˜ëŠ” ë°”ë¥¼ í™•ì‹¤íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”<br>
            ì˜ˆ) "ê³¼ì˜ì¡´ë¥ " â†’ "2024ë…„ ì²­ì†Œë…„ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ìœ„í—˜êµ° ë¹„ìœ¨"
            ì˜ˆ) "ìˆí¼ê³¼ ê³¼ì˜ì¡´" â†’ "ìˆí¼ ì´ìš©ë¥ ì— ë”°ë¥¸ ê³¼ì˜ì¡´ ì°¨ì´" or "ê³¼ì˜ì¡´ìœ„í—˜êµ°ë³„ ìˆí¼ ì´ìš© íŠ¹ì„±ì˜ ì°¨ì´"
        </div>
        <div class="guide-item">
            <strong>âš ï¸ ì£¼ì˜:</strong> AI ë‹µë³€ì— <strong>ì˜¤ë¥˜(í• ë£¨ì‹œë„¤ì´ì…˜)</strong>ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. <br>
            ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”ë¡œ ì¸ìš©í•˜ì§€ ë§ˆì‹œê³ , <strong>ì›ë¬¸ì„ í†µí•´ í•œë²ˆ ë” í™•ì¸í•œ ë’¤</strong> ì •ë³´ë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì‹­ì‹œìš”.
            <a href="https://www.nia.or.kr" target="_blank" style="color: #fff;">NIA í™ˆí˜ì´ì§€</a>ì—ì„œ ì›ë¬¸ í™•ì¸ ê¶Œì¥.
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # =========================================================
    # DB ë‹¤ìš´ë¡œë“œ
    # =========================================================
    if not os.path.exists(LOCAL_DB_PATH) or not os.listdir(LOCAL_DB_PATH):
        st.info("ğŸ”„ Chroma DBë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
        with st.spinner(f"Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘... ({HF_REPO_ID})"):
            db_path, error = download_chroma_db()
        
        if error:
            st.error(f"DB ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {error}")
            st.info("HF_REPO_IDë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return
        else:
            st.success("DB ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            st.rerun()
    
    # =========================================================
    # ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
    # =========================================================
    vectorstore, llms, error = init_resources()
    
    if error:
        st.error(f"ì´ˆê¸°í™” ì˜¤ë¥˜: {error}")
        if "API" in error:
            st.info("Streamlit Secretsì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            with st.form("api_key_form"):
                api_key = st.text_input("OpenAI API í‚¤", type="password")
                submitted = st.form_submit_button("ì„¤ì •")
                if submitted and api_key:
                    os.environ['OPENAI_API_KEY'] = api_key
                    st.rerun()
        return
    
    # =========================================================
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    # =========================================================
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "assistant":
                render_answer_with_tables(message["content"])
            else:
                st.markdown(message["content"])
    
    # =========================================================
    # ì‚¬ìš©ì ì…ë ¥
    # =========================================================
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (ì˜ˆ: 2024ë…„ ì²­ì†Œë…„ ê³¼ì˜ì¡´ë¥ ì€?)"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            status_placeholder = st.empty()
            answer_placeholder = st.empty()
            
            try:
                node_functions = create_node_functions(vectorstore, llms, status_placeholder)
                graph = build_graph(node_functions)
                
                config = {"configurable": {"thread_id": "streamlit_session"}}
                
                result = graph.invoke(
                    {
                        "input": prompt,
                        "chat_history": st.session_state.chat_history,
                        "session_id": "streamlit_session",
                    },
                    config=config
                )
                
                status_placeholder.empty()
                
                final_answer = result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
                with answer_placeholder.container():
                    render_answer_with_tables(final_answer)
                
                # ë””ë²„ê·¸ ì •ë³´
                if debug_mode:
                    with st.expander("ğŸ” ë””ë²„ê·¸ ì •ë³´", expanded=False):
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write(f"**Intent:** {result.get('intent', 'N/A')}")
                            st.write(f"**Followup Type:** {result.get('followup_type', 'N/A')}")
                        with col2:
                            if result.get("plan"):
                                st.write("**Plan:**")
                                st.json(result["plan"])
                        
                        if result.get("retrieval"):
                            st.write(f"**ê²€ìƒ‰ íŒŒì¼:** {result['retrieval'].get('files_searched', [])}")
                            st.write(f"**ë¬¸ì„œ ìˆ˜:** {result['retrieval'].get('doc_count', 0)}")
                
                st.session_state.messages.append({"role": "assistant", "content": final_answer})
                st.session_state.chat_history.append(HumanMessage(content=prompt))
                st.session_state.chat_history.append(AIMessage(content=final_answer))
                
                if len(st.session_state.chat_history) > 20:
                    st.session_state.chat_history = st.session_state.chat_history[-20:]
                
            except Exception as e:
                status_placeholder.empty()
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                import traceback
                if debug_mode:
                    st.code(traceback.format_exc())

if __name__ == "__main__":
    main()




