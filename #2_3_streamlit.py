# =========================================================
# Streamlit ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ RAG ì±—ë´‡
# (Hugging Face Hubì—ì„œ Chroma DB ë‹¤ìš´ë¡œë“œ ë²„ì „)
# =========================================================
import streamlit as st
import json
import re
import os
import pandas as pd
import shutil
from pathlib import Path
from typing import Dict, Any, List, Optional, TypedDict

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# =========================================================
# í˜ì´ì§€ ì„¤ì •
# =========================================================
st.set_page_config(
    page_title="ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ì±—ë´‡",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =========================================================
# ì»¤ìŠ¤í…€ CSS
# =========================================================
st.markdown("""
<style>
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    
    .user-message {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        border-left: 4px solid #1976d2;
    }
    
    .assistant-message {
        background-color: #f5f5f5;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        border-left: 4px solid #424242;
    }
    
    .dataframe {
        font-size: 14px !important;
    }
    
    .status-box {
        background-color: #fff3e0;
        padding: 0.5rem 1rem;
        border-radius: 5px;
        border-left: 4px solid #ff9800;
        margin: 0.5rem 0;
    }
    
    .source-tag {
        background-color: #e8f5e9;
        padding: 0.2rem 0.5rem;
        border-radius: 3px;
        font-size: 0.85em;
        color: #2e7d32;
    }
    
    h1 {
        color: #1a237e;
    }
</style>
""", unsafe_allow_html=True)

# =========================================================
# ìƒìˆ˜ ì„¤ì •
# =========================================================
YEAR_TO_FILENAME = {
    2020: "2020ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°_ì‚¬ë³´ê³ ì„œ.pdf",
    2021: "2021ë…„_ìŠ¤ë§ˆíŠ¸_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³´ê³ ì„œ.pdf",
    2022: "2022ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³´ê³ ì„œ.pdf",
    2023: "2023ë…„_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´ì‹¤íƒœì¡°ì‚¬_ìµœì¢…ë³´ê³ ì„œ.pdf",
    2024: "2024_ìŠ¤ë§ˆíŠ¸í°_ê³¼ì˜ì¡´_ì‹¤íƒœì¡°ì‚¬_ë³¸_ë³´ê³ ì„œ.pdf",
}
ALLOWED_FILES = list(YEAR_TO_FILENAME.values())

BOT_IDENTITY = """2020~2024ë…„ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**ì œê³µ ê°€ëŠ¥í•œ ì •ë³´:**
- ì—°ë„ë³„ ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ìœ„í—˜êµ° ë¹„ìœ¨ ë° ì¶”ì´
- ëŒ€ìƒë³„(ìœ ì•„ë™, ì²­ì†Œë…„, ì„±ì¸, 60ëŒ€) ê³¼ì˜ì¡´ í˜„í™©
- í•™ë ¹ë³„(ì´ˆ/ì¤‘/ê³ /ëŒ€í•™ìƒ) ì„¸ë¶€ ë¶„ì„
- ê³¼ì˜ì¡´ ê´€ë ¨ ìš”ì¸ ë¶„ì„ (SNS, ìˆí¼, ê²Œì„ ì´ìš© ë“±)
- ì¡°ì‚¬ ë°©ë²•ë¡  ë° í‘œë³¸ ì„¤ê³„ ì •ë³´
"""

# =========================================================
# Hugging Face ì„¤ì • - ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”!
# =========================================================
# Hugging Face Dataset ì •ë³´ (ë³¸ì¸ ê²ƒìœ¼ë¡œ ë³€ê²½)
HF_REPO_ID = "Rosaldowithbaek/smartphone-addiction-chroma-db"  # ì˜ˆ: "minseung/smartphone-addiction-chroma-db"
LOCAL_DB_PATH = "./chroma_db_store"

# ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
N_QUERIES = 3
K_PER_QUERY = 6
TOP_PARENTS = 8
TOP_PARENTS_PER_FILE = 2
MAX_CHUNKS_PER_PARENT = 4
MAX_CHARS_PER_DOC = 8000
SUMMARY_TYPES = ["page_summary", "table_summary"]

# =========================================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =========================================================
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "db_downloaded" not in st.session_state:
    st.session_state.db_downloaded = False

# =========================================================
# LangGraph State ì •ì˜
# =========================================================
class GraphState(TypedDict):
    input: str
    chat_history: List[BaseMessage]
    session_id: str
    intent_raw: Optional[str]
    intent: Optional[str]
    is_chat_reference: Optional[bool]
    is_new_topic: Optional[bool]
    plan: Optional[Dict[str, Any]]
    resolved_question: Optional[str]
    previous_context: Optional[str]
    retrieval: Optional[Dict[str, Any]]
    context: Optional[str]
    draft_answer: Optional[str]
    validator_result: Optional[Dict[str, Any]]
    final_answer: Optional[str]
    debug_info: Optional[Dict[str, Any]]

# =========================================================
# Hugging Faceì—ì„œ DB ë‹¤ìš´ë¡œë“œ
# =========================================================
@st.cache_resource
def download_chroma_db():
    """Hugging Face Hubì—ì„œ Chroma DB ë‹¤ìš´ë¡œë“œ"""
    
    # ì´ë¯¸ ë¡œì»¬ì— ìˆìœ¼ë©´ ìŠ¤í‚µ
    if os.path.exists(LOCAL_DB_PATH) and os.listdir(LOCAL_DB_PATH):
        return LOCAL_DB_PATH, None
    
    try:
        from huggingface_hub import snapshot_download
        
        # ë‹¤ìš´ë¡œë“œ
        downloaded_path = snapshot_download(
            repo_id=HF_REPO_ID,
            repo_type="dataset",
            local_dir=LOCAL_DB_PATH,
            local_dir_use_symlinks=False
        )
        
        return downloaded_path, None
        
    except Exception as e:
        return None, str(e)

# =========================================================
# ì´ˆê¸°í™” í•¨ìˆ˜
# =========================================================
@st.cache_resource
def init_resources():
    """ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” (ìºì‹œë¨)"""
    
    # API í‚¤ ì„¤ì •
    api_key = None
    
    # 1. Streamlit secretsì—ì„œ ì‹œë„
    try:
        api_key = st.secrets["OPENAI_API_KEY"]
    except:
        pass
    
    # 2. í™˜ê²½ë³€ìˆ˜ì—ì„œ ì‹œë„
    if not api_key:
        api_key = os.environ.get("OPENAI_API_KEY")
    
    # 3. íŒŒì¼ì—ì„œ ì‹œë„
    if not api_key:
        try:
            with open('openai_api_for_rag_test.txt', 'r') as f:
                api_key = f.read().strip()
        except:
            pass
    
    if not api_key:
        return None, None, "API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    os.environ['OPENAI_API_KEY'] = api_key
    
    # Chroma DB ê²½ë¡œ í™•ì¸
    db_path = LOCAL_DB_PATH
    
    if not os.path.exists(db_path):
        return None, None, f"Chroma DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}"
    
    try:
        embedding = OpenAIEmbeddings(model='text-embedding-3-large')
        vectorstore = Chroma(
            persist_directory=db_path,
            embedding_function=embedding,
            collection_name="pdf_pages_with_summary_v2"
        )
        
        # LLM ì„¤ì •
        llms = {
            "router": ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=10),
            "casual": ChatOpenAI(model="gpt-4o-mini", temperature=0.5, max_tokens=300),
            "main": ChatOpenAI(model="gpt-4o", temperature=0.2, max_tokens=3000),
            "planner": ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=800),
        }
        
        return vectorstore, llms, None
    except Exception as e:
        return None, None, str(e)

# =========================================================
# í—¬í¼ í•¨ìˆ˜ë“¤
# =========================================================
def is_chat_reference_question(user_input: str) -> bool:
    patterns = [
        r"ë‚´\s*ì´ë¦„", r"ì œ\s*ì´ë¦„", r"ë‚˜(ë¥¼|ì˜|í•œí…Œ)", 
        r"ë­ë¼ê³ \s*(í–ˆ|ë¬¼ì–´|ë§)", r"ì•„ê¹Œ", r"ë°©ê¸ˆ", r"ì´ì „ì—",
    ]
    for p in patterns:
        if re.search(p, user_input):
            return True
    return False

def is_new_topic_question(user_input: str, prev_keywords: List[str]) -> bool:
    followup_patterns = [
        r"^ê·¸ëŸ¬ë©´\s", r"^ê·¸ë˜ì„œ\s", r"^ê·¸ê±´\s", r"^ê·¸\s",
        r"ê²°ê³¼ëŠ”\s*\??$", r"ì–´ë•Œ\s*\??$", r"ì–´ë–»ê²Œ\s*(ë¼|ë˜)\s*\??$",
    ]
    for p in followup_patterns:
        if re.search(p, user_input):
            return False
    
    new_topic_keywords = [
        "ìˆí¼", "SNS", "ê²Œì„", "ì´ìš©ì‹œê°„", "ì´ìš©ë¥ ",
        "ê°€êµ¬ì›", "ì†Œë“", "ì§€ì—­", "ì„±ë³„", "ì—°ë ¹",
    ]
    
    input_has_new_topic = any(kw in user_input for kw in new_topic_keywords)
    
    if input_has_new_topic:
        current_topics = [kw for kw in new_topic_keywords if kw in user_input]
        overlap = set(current_topics) & set(prev_keywords)
        if not overlap:
            return True
    
    if len(user_input) > 30 and not any(re.search(p, user_input) for p in followup_patterns):
        return True
    
    return False

def parse_year_range(text: str) -> List[int]:
    years = set()
    
    range_patterns = [
        r"(20[2][0-4])\s*ë…„?\s*(?:ì—ì„œ|ë¶€í„°|~|-|â€“)\s*(20[2][0-4])\s*ë…„?\s*(?:ê¹Œì§€)?",
        r"(20[2][0-4])\s*(?:~|-|â€“)\s*(20[2][0-4])",
    ]
    for pattern in range_patterns:
        matches = re.findall(pattern, text)
        for m in matches:
            start, end = int(m[0]), int(m[1])
            for y in range(start, end + 1):
                if y in YEAR_TO_FILENAME:
                    years.add(y)
    
    single_years = re.findall(r"\b(20[2][0-4])\s*ë…„?\b", text)
    for y in single_years:
        yi = int(y)
        if yi in YEAR_TO_FILENAME:
            years.add(yi)
    
    return sorted(list(years))

def extract_previous_context(chat_history: List[BaseMessage]) -> Dict[str, Any]:
    context = {
        "user_name": None,
        "last_topic": None,
        "last_years": [],
        "last_keywords": [],
    }
    
    if not chat_history:
        return context
    
    for msg in chat_history:
        if isinstance(msg, HumanMessage):
            name_match = re.search(r"(?:ë‚´\s*ì´ë¦„ì€?|ì €ëŠ”?|ë‚˜ëŠ”?)\s*([ê°€-í£a-zA-Z]+)", msg.content)
            if name_match:
                context["user_name"] = name_match.group(1)
    
    recent = chat_history[-4:] if len(chat_history) > 4 else chat_history
    
    for msg in reversed(recent):
        content = msg.content if hasattr(msg, 'content') else str(msg)
        
        years = parse_year_range(content)
        if years and not context["last_years"]:
            context["last_years"] = years
        
        keywords = []
        kw_patterns = [
            r"(ê³¼ì˜ì¡´|ê³¼ì˜ì¡´ë¥ |ìœ„í—˜êµ°|ê³ ìœ„í—˜êµ°)",
            r"(ì²­ì†Œë…„|ìœ ì•„ë™|ì„±ì¸|60ëŒ€|ëŒ€í•™ìƒ|ì¤‘í•™ìƒ|ê³ ë“±í•™ìƒ|ì´ˆë“±í•™ìƒ|í•™ë ¹ë³„|ëŒ€ìƒë³„)",
            r"(SNS|ìˆí¼|ê²Œì„|ìœ íŠœë¸Œ|í‹±í†¡|ì¸ìŠ¤íƒ€)",
            r"(ì´ìš©ë¥ |ì´ìš©ì‹œê°„|ë¹„ìœ¨|ë³€í™”|ì¶”ì´)",
        ]
        for p in kw_patterns:
            found = re.findall(p, content)
            keywords.extend(found)
        
        if keywords and not context["last_keywords"]:
            context["last_keywords"] = list(set(keywords))
        
        if isinstance(msg, HumanMessage) and not context["last_topic"]:
            context["last_topic"] = content[:200]
    
    return context

def _keyword_boost_score(doc: Document, must_terms: List[str]) -> float:
    text = (doc.page_content or "").lower()
    text = re.sub(r"\s+", "", text)
    
    boost = 0.0
    for term in must_terms:
        term_norm = re.sub(r"\s+", "", term.lower())
        if term_norm in text:
            boost += 0.05
    return boost

# =========================================================
# í…Œì´ë¸” íŒŒì‹± ë° ë Œë”ë§
# =========================================================
def parse_markdown_table(text: str) -> List[Dict[str, Any]]:
    """ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ì„ íŒŒì‹±"""
    tables = []
    lines = text.split('\n')
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        
        if line.startswith('|') and line.endswith('|'):
            table_lines = []
            
            while i < len(lines):
                line = lines[i].strip()
                if line.startswith('|') and line.endswith('|'):
                    table_lines.append(line)
                    i += 1
                elif line.startswith('|---') or line.startswith('| ---'):
                    i += 1
                    continue
                else:
                    break
            
            if len(table_lines) >= 2:
                header_line = table_lines[0]
                headers = [h.strip() for h in header_line.split('|')[1:-1]]
                
                data_rows = []
                for row_line in table_lines[1:]:
                    if '---' in row_line:
                        continue
                    cells = [c.strip() for c in row_line.split('|')[1:-1]]
                    if len(cells) == len(headers):
                        data_rows.append(cells)
                
                if headers and data_rows:
                    tables.append({
                        'headers': headers,
                        'rows': data_rows,
                        'start_idx': i - len(table_lines),
                        'end_idx': i
                    })
        else:
            i += 1
    
    return tables

def render_table(headers: List[str], rows: List[List[str]]) -> None:
    """í…Œì´ë¸”ì„ Streamlit DataFrameìœ¼ë¡œ ë Œë”ë§"""
    try:
        df = pd.DataFrame(rows, columns=headers)
        st.dataframe(df, use_container_width=True, hide_index=True)
    except Exception as e:
        st.markdown("| " + " | ".join(headers) + " |")
        st.markdown("| " + " | ".join(["---"] * len(headers)) + " |")
        for row in rows:
            st.markdown("| " + " | ".join(row) + " |")

def render_answer_with_tables(answer: str) -> None:
    """ë‹µë³€ì„ í…Œì´ë¸”ê³¼ í…ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬í•˜ì—¬ ë Œë”ë§"""
    tables = parse_markdown_table(answer)
    
    if not tables:
        st.markdown(answer)
        return
    
    lines = answer.split('\n')
    current_pos = 0
    
    for table in tables:
        before_text = '\n'.join(lines[current_pos:table['start_idx']])
        if before_text.strip():
            st.markdown(before_text)
        
        render_table(table['headers'], table['rows'])
        
        current_pos = table['end_idx']
    
    after_text = '\n'.join(lines[current_pos:])
    if after_text.strip():
        st.markdown(after_text)

# =========================================================
# í”„ë¡¬í”„íŠ¸ ì •ì˜
# =========================================================
def get_router_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.\n"
         "ì´ ì‹œìŠ¤í…œì€ 'ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024)' ì „ë¬¸ RAGì…ë‹ˆë‹¤.\n\n"
         "ë¶„ë¥˜ ê¸°ì¤€:\n"
         "SMALLTALK: ì¸ì‚¬, ê°ì‚¬, ì¡ë‹´, ì‹œìŠ¤í…œ ì†Œê°œ ìš”ì²­\n"
         "RAG: ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì¡°ì‚¬ ê´€ë ¨ ì§ˆë¬¸\n"
         "CHAT_REF: ì´ì „ ëŒ€í™” ë‚´ìš© ì°¸ì¡°\n"
         "OFFTOPIC: ì™„ì „íˆ ê´€ë ¨ ì—†ëŠ” ì£¼ì œ\n\n"
         "ì¶œë ¥: SMALLTALK / RAG / CHAT_REF / OFFTOPIC ì¤‘ í•˜ë‚˜ë§Œ"
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

def get_smalltalk_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024ë…„) ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n\n"
         f"ì‹œìŠ¤í…œ ì—­í• :\n{BOT_IDENTITY}\n\n"
         "ì‘ë‹µ ì§€ì¹¨:\n"
         "- ì¸ì‚¬ì—ëŠ” ê°„ê²°í•˜ê²Œ ì‘ëŒ€í•˜ê³  ì‹œìŠ¤í…œ ì—­í• ì„ ì•ˆë‚´\n"
         "- ì´ëª¨í‹°ì½˜ ì‚¬ìš© ê¸ˆì§€\n"
         "- ê²©ì‹ì²´ ì‚¬ìš© (ìŠµë‹ˆë‹¤/ì…ë‹ˆë‹¤)\n"
         "- 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ"
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

def get_offtopic_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024ë…„) ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n\n"
         f"ì‹œìŠ¤í…œ ì—­í• :\n{BOT_IDENTITY}\n\n"
         "ë„ë©”ì¸ ì™¸ ì§ˆë¬¸ ì‘ëŒ€:\n"
         "- í•´ë‹¹ ì£¼ì œëŠ” ì „ë¬¸ ë¶„ì•¼ê°€ ì•„ë‹˜ì„ ì•ˆë‚´\n"
         "- ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ê´€ë ¨ ì§ˆë¬¸ì€ ë„ì›€ ê°€ëŠ¥í•¨ì„ ì–¸ê¸‰\n"
         "- ê²©ì‹ì²´ ì‚¬ìš©, 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ"
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

def get_planner_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024ë…„) ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ê¸°ì…ë‹ˆë‹¤.\n"
         "ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.\n\n"
         "ì„ë¬´:\n"
         "1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ìê¸°ì™„ê²°í˜•ìœ¼ë¡œ ì¬êµ¬ì„±\n"
         "2. ê²€ìƒ‰ ì¿¼ë¦¬ 3ê°œ ìƒì„±\n"
         "3. í•„ìš”í•œ ì—°ë„/íŒŒì¼ ì‹ë³„\n\n"
         "ìƒˆ ì£¼ì œ vs í›„ì†ì§ˆë¬¸ íŒë‹¨:\n"
         "- is_new_topic=true: ì´ì „ ë§¥ë½ ë¬´ì‹œ\n"
         "- is_new_topic=false: ì´ì „ ë§¥ë½ í™œìš©\n\n"
         "ì—°ë„ ë²”ìœ„ ì²˜ë¦¬:\n"
         "- '2021ë…„ì—ì„œ 2024ë…„ê¹Œì§€' â†’ years: [2021, 2022, 2023, 2024]\n\n"
         "í—ˆìš© íŒŒì¼ëª…:\n" +
         "\n".join([f"- {y}ë…„: {fn}" for y, fn in YEAR_TO_FILENAME.items()]) +
         "\n\nJSON ìŠ¤í‚¤ë§ˆ:\n"
         "{\n"
         '  "resolved_question": "ì™„ì „í•œ ì§ˆë¬¸",\n'
         '  "years": [2020, ...],\n'
         '  "file_name_filters": ["íŒŒì¼ëª…"],\n'
         '  "query_type": "ì¡°ì‚¬ì„¤ê³„" | "ê²°ê³¼/ë¶„ì„",\n'
         '  "must_keep_terms": ["í•µì‹¬ìš©ì–´"],\n'
         '  "queries": ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", "ì¿¼ë¦¬3"]\n'
         "}"
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "í˜„ì¬ ì§ˆë¬¸: {input}\nìƒˆ ì£¼ì œ ì—¬ë¶€: {is_new_topic}\nì´ì „ ë§¥ë½: {prev_context}\n\nJSON:")
    ])

def get_answer_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë³´ê³ ì„œ(2020~2024ë…„) ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n\n"
         "í•µì‹¬ ì›ì¹™:\n"
         "1. CONTEXTì— ìˆëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜/ë¹„ìœ¨ì„ ë°˜ë“œì‹œ ì¸ìš©\n"
         "2. ëª¨ë“  ìˆ˜ì¹˜ì—ëŠ” ì¶œì²˜(íŒŒì¼ëª… p.í˜ì´ì§€) í•„ìˆ˜\n"
         "3. ì—°ë„ë³„ ë¹„êµ ì‹œ ë³€í™”ëŸ‰(%p) ëª…ì‹œ\n"
         "4. ê°ê´€ì ì´ê³  ë‹´ë°±í•œ í†¤ ìœ ì§€\n\n"
         "í˜•ì‹ ê·œì¹™:\n"
         "- í•µì‹¬ ìˆ˜ì¹˜ë¥¼ ë¨¼ì € ì œì‹œ\n"
         "- ì—°ë„ë³„ ë°ì´í„°ëŠ” ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ ì‚¬ìš©\n"
         "- ì´ëª¨í‹°ì½˜ ì‚¬ìš© ê¸ˆì§€\n"
         "- ê²©ì‹ì²´ ì‚¬ìš©\n\n"
         "ì£¼ì˜:\n"
         "- CONTEXTì— ì—†ëŠ” ì—°ë„ëŠ” 'í•´ë‹¹ ì—°ë„ ë°ì´í„°ëŠ” ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'ë¡œ ëª…ì‹œ\n"
         "- ì¶”ì¸¡í•˜ì§€ ì•Šê³  ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€"
        ),
        ("human",
         "[ì§ˆë¬¸]\n{input}\n\n"
         "[ê²€ìƒ‰ ê²°ê³¼]\n{context}\n\n"
         "ìœ„ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.")
    ])

def get_validator_prompt():
    return ChatPromptTemplate.from_messages([
        ("system",
         "í†µê³„ ë³´ê³ ì„œ ë‹µë³€ í’ˆì§ˆ ê²€ìˆ˜ê¸°ì…ë‹ˆë‹¤.\n\n"
         "ê²€ìˆ˜ í•­ëª©:\n"
         "1. ìˆ˜ì¹˜/ë¹„ìœ¨ì— ì¶œì²˜ ìˆëŠ”ì§€\n"
         "2. CONTEXTì— ì—†ëŠ” ìˆ˜ì¹˜ë¥¼ ìƒì„±í–ˆëŠ”ì§€\n"
         "3. ì§ˆë¬¸ì—ì„œ ìš”ì²­í•œ ì—°ë„/í•­ëª©ì„ ëª¨ë‘ ë‹¤ë¤˜ëŠ”ì§€\n\n"
         "JSONë§Œ ì¶œë ¥:\n"
         "{\n"
         '  "needs_fix": true|false,\n'
         '  "issues": ["ë¬¸ì œì "],\n'
         '  "corrected_answer": "ìˆ˜ì •ëœ ë‹µë³€ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´"\n'
         "}"
        ),
        ("human",
         "[ì§ˆë¬¸]\n{input}\n\n"
         "[ê²€ìƒ‰ ê²°ê³¼]\n{context}\n\n"
         "[ë‹µë³€]\n{answer}\n\n"
         "JSON:")
    ])

# =========================================================
# ë…¸ë“œ í•¨ìˆ˜ë“¤
# =========================================================
def create_node_functions(vectorstore, llms, status_placeholder):
    """ë…¸ë“œ í•¨ìˆ˜ë“¤ì„ ìƒì„±í•˜ê³  ë°˜í™˜"""
    
    def update_status(message: str):
        status_placeholder.markdown(f"""
        <div style="background-color: #fff3e0; padding: 0.8rem 1rem; border-radius: 8px; 
                    border-left: 4px solid #ff9800; margin: 0.5rem 0;">
            <span style="font-weight: 500;">ğŸ”„ {message}</span>
        </div>
        """, unsafe_allow_html=True)
    
    def route_intent(state: GraphState) -> GraphState:
        update_status("ì§ˆë¬¸ ë¶„ì„ ì¤‘...")
        
        try:
            user_input = state["input"]
            chat_history = state.get("chat_history", [])
            
            if is_chat_reference_question(user_input):
                state["intent_raw"] = "CHAT_REF"
                state["intent"] = "CHAT_REF"
                state["is_chat_reference"] = True
                return state
            
            prev_ctx = extract_previous_context(chat_history)
            state["is_new_topic"] = is_new_topic_question(user_input, prev_ctx.get("last_keywords", []))
            
            result = (get_router_prompt() | llms["router"] | StrOutputParser()).invoke({
                "input": user_input,
                "chat_history": chat_history
            })
            state["intent_raw"] = result.strip().upper()
            
            if re.search(r"\b(20[2][0-4])\s*ë…„?\b", user_input):
                state["intent"] = "RAG"
                return state
            
            rag_keywords = [
                "ê³¼ì˜ì¡´", "ìŠ¤ë§ˆíŠ¸í°", "ì¡°ì‚¬", "ì‹¤íƒœ", "ë¹„ìœ¨", "ë¥ ", "%",
                "í†µê³„", "ìˆ˜ì¹˜", "ê²°ê³¼", "ì²­ì†Œë…„", "ëŒ€í•™ìƒ", "ì„±ì¸",
                "ìˆí¼", "SNS", "ê²Œì„", "ì´ìš©ë¥ ", "ìœ„í—˜êµ°"
            ]
            if any(kw in user_input for kw in rag_keywords):
                state["intent"] = "RAG"
                return state
            
            if state["intent_raw"] in ("SMALLTALK", "RAG", "OFFTOPIC", "CHAT_REF"):
                state["intent"] = state["intent_raw"]
            else:
                state["intent"] = "RAG"
            
            return state
        except Exception as e:
            state["intent"] = "RAG"
            return state
    
    def handle_smalltalk(state: GraphState) -> GraphState:
        update_status("ì‘ë‹µ ìƒì„± ì¤‘...")
        try:
            answer = (get_smalltalk_prompt() | llms["casual"] | StrOutputParser()).invoke({
                "input": state["input"],
                "chat_history": state.get("chat_history", [])
            })
            state["final_answer"] = answer
            return state
        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state
    
    def handle_offtopic(state: GraphState) -> GraphState:
        update_status("ì‘ë‹µ ìƒì„± ì¤‘...")
        try:
            answer = (get_offtopic_prompt() | llms["casual"] | StrOutputParser()).invoke({
                "input": state["input"],
                "chat_history": state.get("chat_history", [])
            })
            state["final_answer"] = answer
            return state
        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state
    
    def handle_chat_reference(state: GraphState) -> GraphState:
        update_status("ëŒ€í™” ê¸°ë¡ í™•ì¸ ì¤‘...")
        try:
            chat_history = state.get("chat_history", [])
            user_input = state["input"]
            prev_ctx = extract_previous_context(chat_history)
            
            if re.search(r"(ë‚´|ì œ)\s*ì´ë¦„", user_input):
                if prev_ctx["user_name"]:
                    state["final_answer"] = f"{prev_ctx['user_name']}ë‹˜ìœ¼ë¡œ ë§ì”€í•˜ì…¨ìŠµë‹ˆë‹¤."
                else:
                    state["final_answer"] = "ì•„ì§ ì´ë¦„ì„ ë§ì”€í•´ì£¼ì‹œì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                return state
            
            if re.search(r"(ë­ë¼ê³ |ë¬´ìŠ¨\s*ë§|ë­\s*ë¬¼ì–´)", user_input):
                if prev_ctx["last_topic"]:
                    state["final_answer"] = f"ì´ì „ì— '{prev_ctx['last_topic'][:80]}...'ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì…¨ìŠµë‹ˆë‹¤."
                else:
                    state["final_answer"] = "ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                return state
            
            state["final_answer"] = "ì´ì „ ëŒ€í™” ì°¸ì¡°ê°€ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ë§ì”€í•´ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"
            return state
        except Exception as e:
            state["final_answer"] = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state
    
    def plan_search(state: GraphState) -> GraphState:
        update_status("ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ ì¤‘...")
        try:
            user_input = state["input"]
            chat_history = state.get("chat_history", [])
            is_new_topic = state.get("is_new_topic", True)
            
            prev_ctx = extract_previous_context(chat_history)
            
            if is_new_topic:
                prev_context_str = "ìƒˆë¡œìš´ ì£¼ì œ - ì´ì „ ë§¥ë½ ë¬´ì‹œ"
            else:
                prev_context_str = ""
                if prev_ctx["last_topic"]:
                    prev_context_str += f"ì´ì „ ì£¼ì œ: {prev_ctx['last_topic'][:100]}\n"
                if prev_ctx["last_years"]:
                    prev_context_str += f"ì´ì „ ì—°ë„: {prev_ctx['last_years']}\n"
                if prev_ctx["last_keywords"]:
                    prev_context_str += f"ì´ì „ í‚¤ì›Œë“œ: {prev_ctx['last_keywords']}"
                if not prev_context_str:
                    prev_context_str = "ì—†ìŒ"
            
            state["previous_context"] = prev_context_str
            
            result = (get_planner_prompt() | llms["planner"] | StrOutputParser()).invoke({
                "input": user_input,
                "chat_history": chat_history,
                "is_new_topic": str(is_new_topic),
                "prev_context": prev_context_str
            })
            
            json_match = re.search(r'\{[\s\S]*\}', result)
            if json_match:
                result = json_match.group()
            
            plan = json.loads(result)
            
            years = plan.get('years', [])
            if not isinstance(years, list):
                years = []
            
            input_years = parse_year_range(user_input)
            years = list(set(years + input_years))
            years = [y for y in years if isinstance(y, int) and y in YEAR_TO_FILENAME]
            years = sorted(years)
            
            if not years and not is_new_topic and prev_ctx["last_years"]:
                years = prev_ctx["last_years"]
            
            fns = plan.get("file_name_filters", [])
            if not isinstance(fns, list):
                fns = []
            fns = [fn for fn in fns if isinstance(fn, str) and fn in ALLOWED_FILES]
            
            if years and not fns:
                fns = [YEAR_TO_FILENAME[y] for y in years if y in YEAR_TO_FILENAME]
            
            queries = plan.get('queries', [])
            if not isinstance(queries, list):
                queries = []
            queries = [str(q).strip() for q in queries if str(q).strip()]
            
            resolved_q = plan.get("resolved_question", "")
            if not isinstance(resolved_q, str):
                resolved_q = ""
            resolved_q = resolved_q.strip()
            
            if len(resolved_q) < 15 and not is_new_topic and prev_ctx["last_keywords"]:
                keywords_str = " ".join(prev_ctx["last_keywords"])
                resolved_q = f"{keywords_str} {resolved_q}".strip()
            
            fallback_q = resolved_q or user_input
            while len(queries) < N_QUERIES:
                queries.append(fallback_q)
            if len(queries) > N_QUERIES:
                queries = queries[:N_QUERIES]
            
            keep = plan.get('must_keep_terms', [])
            if not isinstance(keep, list):
                keep = []
            keep = [str(x).strip() for x in keep if str(x).strip()]
            
            if not is_new_topic and prev_ctx["last_keywords"]:
                keep = list(set(keep + prev_ctx["last_keywords"]))
            
            state["plan"] = {
                "years": years,
                "file_name_filters": fns,
                "query_type": plan.get('query_type', "ê²°ê³¼/ë¶„ì„"),
                "must_keep_terms": keep,
                "queries": queries,
                "resolved_question": resolved_q,
            }
            state["resolved_question"] = resolved_q
            
            return state
            
        except Exception as e:
            is_new_topic = state.get("is_new_topic", True)
            prev_ctx = extract_previous_context(state.get("chat_history", []))
            fallback_years = parse_year_range(state["input"])
            
            if not fallback_years and not is_new_topic and prev_ctx["last_years"]:
                fallback_years = prev_ctx["last_years"]
            
            fallback_fns = [YEAR_TO_FILENAME[y] for y in fallback_years if y in YEAR_TO_FILENAME]
            
            resolved = state["input"]
            
            state["plan"] = {
                "years": fallback_years,
                "file_name_filters": fallback_fns,
                "query_type": "ê²°ê³¼/ë¶„ì„",
                "must_keep_terms": [] if is_new_topic else prev_ctx.get("last_keywords", []),
                "queries": [resolved] * N_QUERIES,
                "resolved_question": resolved,
            }
            state["resolved_question"] = resolved
            return state
    
    def retrieve_documents(state: GraphState) -> GraphState:
        update_status("ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘...")
        try:
            plan = state["plan"]
            target_files = plan.get("file_name_filters", [])
            queries = plan.get("queries", [])
            must_terms = plan.get("must_keep_terms", [])
            
            all_docs = []
            
            if target_files:
                for fn in target_files:
                    file_filter = {'$and': [
                        {'doc_type': {"$in": SUMMARY_TYPES}},
                        {'file_name': fn}
                    ]}
                    
                    file_docs = []
                    seen_keys = set()
                    
                    for q in queries:
                        if not q:
                            continue
                        hits = vectorstore.similarity_search_with_relevance_scores(
                            q, k=K_PER_QUERY, filter=file_filter
                        )
                        for doc, score in hits:
                            key = f"{doc.metadata.get('parent_id')}|{doc.metadata.get('page')}"
                            if key in seen_keys:
                                continue
                            doc.metadata["_score"] = float(score)
                            doc.metadata["_source_file"] = fn
                            file_docs.append(doc)
                            seen_keys.add(key)
                    
                    for doc in file_docs:
                        base_score = doc.metadata.get("_score", 0.0)
                        boost = _keyword_boost_score(doc, must_terms)
                        doc.metadata["_final_score"] = base_score + boost
                    
                    file_docs.sort(key=lambda d: d.metadata.get("_final_score", 0.0), reverse=True)
                    all_docs.extend(file_docs[:TOP_PARENTS_PER_FILE * 2])
            else:
                base_filter = {'doc_type': {"$in": SUMMARY_TYPES}}
                seen_keys = set()
                
                for q in queries:
                    if not q:
                        continue
                    hits = vectorstore.similarity_search_with_relevance_scores(
                        q, k=K_PER_QUERY, filter=base_filter
                    )
                    for doc, score in hits:
                        key = f"{doc.metadata.get('parent_id')}|{doc.metadata.get('page')}"
                        if key in seen_keys:
                            continue
                        doc.metadata["_score"] = float(score)
                        all_docs.append(doc)
                        seen_keys.add(key)
                
                for doc in all_docs:
                    base_score = doc.metadata.get("_score", 0.0)
                    boost = _keyword_boost_score(doc, must_terms)
                    doc.metadata["_final_score"] = base_score + boost
            
            all_docs.sort(key=lambda d: d.metadata.get("_final_score", 0.0), reverse=True)
            
            parent_ids = []
            seen_pid = set()
            
            if target_files:
                for fn in target_files:
                    for doc in all_docs:
                        if doc.metadata.get("file_name") != fn:
                            continue
                        pid = doc.metadata.get("parent_id")
                        if pid and pid not in seen_pid:
                            parent_ids.append(pid)
                            seen_pid.add(pid)
                            break
                
                for doc in all_docs:
                    if len(parent_ids) >= TOP_PARENTS:
                        break
                    pid = doc.metadata.get("parent_id")
                    if pid and pid not in seen_pid:
                        parent_ids.append(pid)
                        seen_pid.add(pid)
            else:
                for doc in all_docs:
                    pid = doc.metadata.get("parent_id")
                    if not pid or pid in seen_pid:
                        continue
                    parent_ids.append(pid)
                    seen_pid.add(pid)
                    if len(parent_ids) >= TOP_PARENTS:
                        break
            
            expanded_chunks = []
            for pid in parent_ids:
                got = vectorstore._collection.get(
                    where={'parent_id': pid},
                    include=['documents', 'metadatas']
                )
                docs = got.get("documents", []) or []
                metas = got.get("metadatas", []) or []
                
                chunks = []
                for txt, meta in zip(docs, metas):
                    if not isinstance(meta, dict):
                        continue
                    if meta.get("doc_type") != "text_chunk":
                        continue
                    idx = int(meta.get("chunk_index", 0))
                    chunks.append((idx, txt or "", meta))
                
                chunks.sort(key=lambda x: x[0])
                for idx, txt, meta in chunks[:MAX_CHUNKS_PER_PARENT]:
                    expanded_chunks.append(Document(page_content=txt, metadata=meta))
            
            pid_set = set(parent_ids)
            kept_summaries = [d for d in all_docs if d.metadata.get("parent_id") in pid_set]
            final_docs = kept_summaries + expanded_chunks
            
            blocks = []
            for i, d in enumerate(final_docs, start=1):
                m = d.metadata
                text = d.page_content[:MAX_CHARS_PER_DOC]
                blocks.append(
                    f"[{i}] {m.get('file_name', 'unknown')} (p.{m.get('page', '?')})\n{text}"
                )
            context = "\n\n---\n\n".join(blocks)
            
            state["retrieval"] = {
                "docs": final_docs,
                "parent_ids": parent_ids,
                "files_searched": target_files or ["ì „ì²´"],
            }
            state["context"] = context
            
            return state
            
        except Exception as e:
            state["context"] = ""
            return state
    
    def generate_answer(state: GraphState) -> GraphState:
        update_status("ë‹µë³€ ìƒì„± ì¤‘...")
        try:
            answer = (get_answer_prompt() | llms["main"] | StrOutputParser()).invoke({
                "input": state["resolved_question"] or state["input"],
                "context": state.get("context", "")
            })
            state["draft_answer"] = answer
            return state
        except Exception as e:
            state["draft_answer"] = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            return state
    
    def validate_answer(state: GraphState) -> GraphState:
        update_status("ë‹µë³€ ê²€ì¦ ì¤‘...")
        try:
            result = (get_validator_prompt() | llms["main"] | StrOutputParser()).invoke({
                "input": state["resolved_question"] or state["input"],
                "context": state.get("context", ""),
                "answer": state["draft_answer"]
            })
            
            json_match = re.search(r'\{[\s\S]*\}', result)
            if json_match:
                result = json_match.group()
            
            validator_out = json.loads(result)
            state["validator_result"] = validator_out
            
            if validator_out.get("needs_fix") and validator_out.get("corrected_answer"):
                state["final_answer"] = validator_out["corrected_answer"]
            else:
                state["final_answer"] = state["draft_answer"]
            
            return state
            
        except Exception as e:
            state["final_answer"] = state["draft_answer"]
            return state
    
    def handle_clarify(state: GraphState) -> GraphState:
        clarify_msg = state["resolved_question"].replace("CLARIFY:", "", 1).strip()
        state["final_answer"] = clarify_msg
        return state
    
    return {
        "route_intent": route_intent,
        "smalltalk": handle_smalltalk,
        "offtopic": handle_offtopic,
        "chat_ref": handle_chat_reference,
        "plan_search": plan_search,
        "retrieve": retrieve_documents,
        "generate": generate_answer,
        "validate": validate_answer,
        "clarify": handle_clarify,
    }

# =========================================================
# ê·¸ë˜í”„ ë¹Œë”
# =========================================================
def build_graph(node_functions):
    """LangGraph ë¹Œë“œ"""
    workflow = StateGraph(GraphState)
    
    for name, func in node_functions.items():
        workflow.add_node(name, func)
    
    def route_by_intent(state: GraphState) -> str:
        intent = state.get("intent", "RAG")
        if intent == "SMALLTALK":
            return "smalltalk"
        elif intent == "OFFTOPIC":
            return "offtopic"
        elif intent == "CHAT_REF":
            return "chat_ref"
        else:
            return "rag_pipeline"
    
    def check_clarify(state: GraphState) -> str:
        resolved = state.get("resolved_question", "")
        if resolved.startswith("CLARIFY:"):
            return "clarify"
        return "retrieve"
    
    workflow.set_entry_point("route_intent")
    
    workflow.add_conditional_edges(
        "route_intent",
        route_by_intent,
        {
            "smalltalk": "smalltalk",
            "offtopic": "offtopic",
            "chat_ref": "chat_ref",
            "rag_pipeline": "plan_search"
        }
    )
    
    workflow.add_edge("smalltalk", END)
    workflow.add_edge("offtopic", END)
    workflow.add_edge("chat_ref", END)
    
    workflow.add_conditional_edges(
        "plan_search",
        check_clarify,
        {
            "clarify": "clarify",
            "retrieve": "retrieve"
        }
    )
    
    workflow.add_edge("clarify", END)
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", "validate")
    workflow.add_edge("validate", END)
    
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)

# =========================================================
# ë©”ì¸ UI
# =========================================================
def main():
    # í—¤ë”
    st.title("ğŸ“Š ìŠ¤ë§ˆíŠ¸í° ê³¼ì˜ì¡´ ì‹¤íƒœì¡°ì‚¬ ë¶„ì„ ì‹œìŠ¤í…œ")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ì‹œìŠ¤í…œ ì •ë³´")
        st.markdown(BOT_IDENTITY)
        
        st.divider()
        
        st.subheader("ë°ì´í„° ë²”ìœ„")
        for year, filename in YEAR_TO_FILENAME.items():
            st.caption(f"â€¢ {year}ë…„")
        
        st.divider()
        
        if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.messages = []
            st.session_state.chat_history = []
            st.rerun()
        
        st.divider()
        
        debug_mode = st.checkbox("ë””ë²„ê·¸ ëª¨ë“œ", value=False)
        
        st.divider()
        st.caption(f"DB ê²½ë¡œ: {LOCAL_DB_PATH}")
        st.caption(f"HF Repo: {HF_REPO_ID}")
    
    # =========================================================
    # DB ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
    # =========================================================
    if not os.path.exists(LOCAL_DB_PATH) or not os.listdir(LOCAL_DB_PATH):
        st.info("ğŸ”„ Chroma DBë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
        
        with st.spinner("Hugging Faceì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë‹¤ìš´ë¡œë“œ ì¤‘..."):
            db_path, error = download_chroma_db()
        
        if error:
            st.error(f"DB ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {error}")
            st.info("HF_REPO_IDë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return
        else:
            st.success("DB ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            st.rerun()
    
    # =========================================================
    # ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
    # =========================================================
    vectorstore, llms, error = init_resources()
    
    if error:
        st.error(f"ì´ˆê¸°í™” ì˜¤ë¥˜: {error}")
        
        if "API" in error:
            st.info("OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            with st.form("api_key_form"):
                api_key = st.text_input("OpenAI API í‚¤", type="password")
                submitted = st.form_submit_button("ì„¤ì •")
                if submitted and api_key:
                    os.environ['OPENAI_API_KEY'] = api_key
                    st.rerun()
        return
    
    # =========================================================
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    # =========================================================
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "assistant":
                render_answer_with_tables(message["content"])
            else:
                st.markdown(message["content"])
    
    # =========================================================
    # ì‚¬ìš©ì ì…ë ¥
    # =========================================================
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            status_placeholder = st.empty()
            answer_placeholder = st.empty()
            
            try:
                node_functions = create_node_functions(vectorstore, llms, status_placeholder)
                graph = build_graph(node_functions)
                
                config = {"configurable": {"thread_id": "streamlit_session"}}
                
                result = graph.invoke(
                    {
                        "input": prompt,
                        "chat_history": st.session_state.chat_history,
                        "session_id": "streamlit_session",
                    },
                    config=config
                )
                
                status_placeholder.empty()
                
                final_answer = result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
                with answer_placeholder.container():
                    render_answer_with_tables(final_answer)
                
                if debug_mode:
                    with st.expander("ğŸ” ë””ë²„ê·¸ ì •ë³´", expanded=False):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.subheader("Intent")
                            st.write(f"ë¶„ë¥˜: {result.get('intent', 'N/A')}")
                            st.write(f"ìƒˆ ì£¼ì œ: {result.get('is_new_topic', 'N/A')}")
                        
                        with col2:
                            if result.get("plan"):
                                st.subheader("Plan")
                                st.json(result["plan"])
                        
                        if result.get("retrieval"):
                            st.subheader("Retrieval")
                            st.write(f"ê²€ìƒ‰ íŒŒì¼: {result['retrieval'].get('files_searched', [])}")
                            st.write(f"ë¬¸ì„œ ìˆ˜: {len(result['retrieval'].get('docs', []))}")
                
                st.session_state.messages.append({"role": "assistant", "content": final_answer})
                st.session_state.chat_history.append(HumanMessage(content=prompt))
                st.session_state.chat_history.append(AIMessage(content=final_answer))
                
                if len(st.session_state.chat_history) > 20:
                    st.session_state.chat_history = st.session_state.chat_history[-20:]
                
            except Exception as e:
                status_placeholder.empty()
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                import traceback
                if debug_mode:
                    st.code(traceback.format_exc())

if __name__ == "__main__":
    main()
